---
title: "Core Stats: Categorical Predictors: ANOVA"
output: html_notebook
---

# Categorical Predictors

## ANOVA

How do I analyse multiple samples of continuous data?
What is an ANOVA?
How do I check for differences between groups?

```{r}
library(tidyverse)
library(rstatix) # Converts stats functions to a tidyverse-friendly format
library(ggResidpanel) # Creates diagnostic plots using ggplot2
```



### 1. Purpose and aim
Analysis of variance or ANOVA is a test than can be used when we have multiple samples of continuous response data. Whilst it is possible to use ANOVA with only two samples, it is generally used when we have three or more groups. It is used to find out if the samples came from parent distributions with the same mean. It can be thought of as a generalization of the two-sample Student’s t-test.



### 2. Data and hypothesis

For example, suppose we measure the feeding rate of oyster catchers (shellfish per hour) at three sites characterized by their degree of shelter from the wind, called "exposed" (E), "partially sheltered" (P), and "sheltered" (S). We want to test whether the data support the hypothesis that feeding rates don’t differ between locations. We form the following null and alternative hypotheses:

$H_0:$ The mean feeding rates at all three sites is the same  $\mu E = \mu P = \mu S$
$H_1:$ The mean feeding rates are not all equal.

Because there are more than two groups (and we don't know any better yet wrt assumptions) we will use ANOVA test, and because we only have one predictor variable (the categorical variable location) we will use a *one-way* ANOVA test.

```{r}
#load the data
oystercatcher <- read_csv("../../data/CS2-oystercatcher-feeding.csv")

#load libraries
library(tidyverse)
library(rstatix) # Converts stats functions to a tidyverse-friendly format
library(ggResidpanel) # Creates diagnostic plots using ggplot2

head(oystercatcher)
```

The data set contains two columns: "site" which has info on the shelter of the feeding location, and "feeding", which has feeding rates. Lets get some descriptive statistics:

```{r}
#get descriptive statistics

oystercatcher %>%
  group_by(site) %>%
  get_summary_stats(type = "common")
```

We see that each shelter site has a sample size of 40.

We can also plot the data by "site":
```{r}
#plot the data
ggplot(oystercatcher,
       aes(x = site, y= feeding)) +
  geom_boxplot()
```

Looking at the data, there appears to be a noticeable difference in feeding rates between the three sites. We would probably expect a reasonably significant statistical result here.



### 3. Assumptions
To use an ANOVA test, we have to make three assumptions:

1) The parent distributions from which the samples are taken are normally distributed
2) Each data point in the samples is independent of the others
3) The parent distributions should have the same variance

In a similar way to the two-sample tests we will consider the normality and equality of variance assumptions both using tests and by graphical inspection (and ignore the independence assumption).

#### 3.1 Normality
First we perform Shapiro-Wilk test on each site separately, as we did on the One-Sample Data discussion. Remember, the Shapiro-Wilk test is quite sensitive to sample size. This means that if you have a large sample then even small deviations from normality will cause the sample to fail the test, whereas smaller samples are allowed to pass with much larger deviations.
```{r}
#Shapiro-Wilk test on each site. We will filter for each type of 'site', extract the 'feeding' rates and send those data to the shapiro.test() function

oystercatcher %>%
  filter(site == "exposed") %>%
  pull(feeding) %>%
  shapiro.test()

oystercatcher %>%
  filter(site == "partial") %>%
  pull(feeding) %>%
  shapiro.test()

oystercatcher %>%
  filter(site == "sheltered") %>%
  pull(feeding) %>%
  shapiro.test()
```
Remember that the p-value gives the probability of observing each sample if the parent population is actually normally distributed. So here we can't reject the null that it is normally distributed. However, also don't forget Shapiro-Wilk test is quite sensitive to sample size. This means that if you have a large sample then even small deviations from normality will cause the sample to fail the test, whereas smaller samples are allowed to pass with much larger deviations.

*Important* to note that for ANOVA, onsidering each group in turn is often considered quite excessive and, in most cases, it is sufficient to consider the normality of the combined set of residuals from the data. To get hold of these residuals, we need to create a linear model.

So we create a linear model, extract the residuals and check their normality:
```{r}
#create a linear model (lm) where the feeding rate depends on the site using oystercatcher data
lm_oystercatcher <- lm(feeding ~ site, 
                       data = oystercatcher)
```

Now we extract the residuals from this object we created (lm_oystercatcher) and use this in the shapiro.test() function:

```{r}
#extract residuals
resid_oyster <- residuals(lm_oystercatcher)

#perform Shapiro-Wilk test on residuals
shapiro.test(resid_oyster)
```

We can again see that the combined residuals from all three groups appear to be normally distributed, which is what we'd expect since they all seemed to be normally distributed individually.

#### 3.2. Homoskedasticity (equality of variance)
Since all the individual groups are normally distributed, we can test for homoskedasticity using Bartlett's test. The reason why we’re checking for equality of variance (also referred to as homogeneity of variance) is because many statistical tests assume that the spread of the data within different parental populations is the same. If that is indeed the case, then the data themselves should have equal spread as well.

```{r}
#check equality of variance
bartlett.test(feeding ~ site,
              data = oystercatcher)
```

Based on the p-value of 0.8624, we see that each group appears to have comparable variance.

#### 3.3 Graphical interpretation and diagnostic plots

Assessing assumptions via these tests can be cumbersome, but also a bit misleading at times. It reduces the answer to the question “is the assumption met?” to a yes/no, based on some statistic and associated p-value.

This does not convey that things aren’t always so clear-cut and that there is a lot of grey area that we need to navigate. As such, assessing assumptions through graphical means - using diagnostic plots - is often preferred.

There is an option to create ggplot-friendly diagnostic plots, using the ggResidPanel package.

Let’s create the diagnostic plots we’re interested in using ggResidPanel. It takes a linear model object as input (lm_oystercatcher) and you can define which plots you want to see using the plots = argument. We are also adding a smoother line (smoother = TRUE) to the plots, which we’ll use to compare against.

```{r}
resid_panel(lm_oystercatcher,
            plots = c("resid", "qq", "ls", "cookd"),
            smoother = TRUE)
```
The top left graph plots the Residuals plot. If the data are best explained by a linear line then there should be a uniform distribution of points above and below the horizontal blue line (and if there are sufficient points then the red line, which is a smoother line, should be on top of the blue line). This plot looks pretty good.
The top right graph shows the Q-Q plot which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal blue line. This plot looks good.
The bottom left Location-Scale graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.
The last graph shows the Cook’s distance and tests if any one point has an unnecessarily large effect on the fit. A rule of thumb is that if any value is larger than 1.0, then it might have a large effect on the model. If not, then no point has undue influence. This plot is good. There are different ways to determine the threshold (apart from simply setting it to 1) and in this plot the blue dashed line is at 4/n, with n being the number of samples. At this threshold there are some data points that may be influential, but I personally find this threshold rather strict.

We can see that these graphs are very much in line with what we’ve just looked at using the test, which is reassuring. The groups all appear to have the same spread of data, and the Q-Q plot shows that the assumption of normality is alright.

Note: At this stage, I should point out that I nearly always stick with the graphical method for assessing the assumptions of a test. Assumptions are rarely either completely met or not met and there is always some degree of personal assessment.

Whilst the formal statistical tests (like Shapiro-Wilk) are technically fine, they can often create a false sense of things being absolutely right or wrong in spite of the fact that they themselves are still probabilistic statistical tests. In these exercises we are using both approaches whilst you gain confidence and experience in interpreting the graphical output and whilst it is absolutely fine to use both in the future I would strongly recommend that you don’t rely solely on the statistical tests in isolation.



### 4. Implement and interpret the test

As is often the case, performing the actual statistical test is the least of our efforts. In R we perform the ANOVA on the linear model object, lm_oystercatcher in this case. This takes the linear model (i.e. finds the means of the three groups and calculates a load of intermediary data that we need for the statistical analysis) and stores this information in an R object (which we’ve called lm_oystercatcher).
```{r}
anova(lm_oystercatcher)
```

In the output:

The 1st line just tells you the that this is an ANOVA test
The 2nd line tells you what the response variable is (in this case feeding)
The 3rd, 4th and 5th lines are an ANOVA table which contain some useful values:
The Df column contains the degrees of freedom values on each row, 2 and 117 (which we can use for the reporting)
The F value column contains the F statistic, 150.78 (which again we’ll need for reporting).
The p-value is 2.2e-16 and is the number directly under the Pr(>F) on the 4th line (to be precise, it is 4.13e-33 but anything smaller than 2.2e-16 gets reported as < 2.2e-16).
The other values in the table (in the Sum Sq and Mean Sq) columns are used to calculate the F statistic itself and we don’t need to know these.
The 6th line has some symbolic codes to represent how big (small) the p-value is; so, a p-value smaller than 0.001 would have a *** symbol next to it (which ours does). Whereas if the p-value was between 0.01 and 0.05 then there would simply be a * character next to it, etc.

Since the p-value is very small (much smaller than the standard significance level of 0.05) we can say “that it is very unlikely that these three samples came from the same parent distribution” and as such we can reject our null hypothesis and state that:

A one-way ANOVA showed that the mean feeding rate of oystercatchers differed significantly between locations (p = 4.13e-33).

One drawback with using an ANOVA test is that it only tests to see if all of the means are the same. If we get a significant result using ANOVA then all we can say is that not all of the means are the same. To test for signficant differences between group means we can use Tukey's range test.



### 5. Summary

We use an ANOVA to test if there is a difference in means between multiple continuous response variables
We check assumptions with diagnostic plots and check if the residuals are normally distributed
We use post-hoc testing to check for significant differences between the group means, for example using Tukey’s range test
